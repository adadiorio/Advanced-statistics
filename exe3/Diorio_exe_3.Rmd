---
title: "Exercise_3_Adv_statistics"
author: "Ada D'Iorio"
output: 
html_document: 
number_sections: true 
theme: spacelab 
pdf_document:
    latex_engine: xelatex
date: "2024-05-04"
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

$\textbf{Exercise 1: Bayesian Inference for Poisson Model}$

The number of particles emitted by a radioactive source during a fixed
interval of time ($\Delta \ t \ = \ 10 \ s$) follows a Poisson
distribution on the parameter $\mu$. The number of particles observed
during consecutive time intervals is: 4, 1, 3, 1, 5, and 3.

(a) Assuming a $\textit{positive uniform}$ prior distribution for the
    parameter $\mu$:

-   Determine and draw the posterior distribution for $\mu$, given the
    data;
-   Evaluate mean, median and variance, both analytically and
    numerically in R.

```{r}
library(ggplot2)

## Introducing the observed data 

N_par <- c(4, 1, 3, 1, 5, 3)
Dt <- 10 #s
lambda <- length(N_par) #rate del processo poissoniano
N <- sum(N_par)+1


## Defining the likelihood, it is the function that represents the probability of measuring a given value of D before taking the measure 

likelihood <- function(mu, data) {
  prod(dpois(data, lambda = mu))
}

mu_values <- seq(0, 10, by = 0.01)

posterior_un <- sapply(mu_values, likelihood, data = N_par)

posterior <- posterior_un / sum(posterior_un)


## ----------------------- Calcolo analitico ------------------------------------

## The product of N poissonian distributions is an erlang distribution, we have to 
## use these parameters in order to compute the calculus 


mean_erlang <- N / lambda 
median <- as.integer((lambda +1 ) / 2)
median_value <- N_par[median]
var_erlang <- N/lambda^2

paste(sprintf('Posterior Mean (Analytical): %.2f', mean_erlang))
paste(sprintf('Posterior Median (Analytical): %.2f', median_value))
paste(sprintf('Posterior Variance (Analytical): %.2f', var_erlang))
cat('------------------------------------------------- \n')

## ------------------------  Calcolo numerico  ----------------------------------

mean_numerical <- sum(mu_values * posterior) # calcolo media numerica (integrazione)

median_numerical <- quantile(N_par, probs = 0.5)[[1]]

variance_numerical <- sum((mu_values^2 * posterior)) - mean_numerical^2


paste(sprintf('Posterior Mean (Numerical): %.2f', mean_numerical))
paste(sprintf('Posterior Median (Numerical): %.2f', median_numerical))
paste(sprintf('Posterior Variance (Numerical): %.2f', variance_numerical))

# --------------------------------------------------------------------------------

df <- data.frame(mu = mu_values, posterior = posterior)

ggplot(df, aes(x = mu, y = posterior)) +
  geom_point(color = 'firebrick', size = 0.7, alpha = 0.9, shape = 15) +
  geom_histogram(fill = "salmon", stat = 'identity', alpha = 0.2) +  
  labs(x = expression(mu),
       y = 'Posterior Probability Density',
       title = "Posterior Distribution (Positive Uniform Prior)") +  
  xlim(0, 10) +
  ylim(0, max(posterior * 1.1)) +  # Use 'df$posterior' instead of just 'posterior'
  theme_light() +  
  theme(panel.background = element_rect(fill = "white")) +  
  geom_vline(xintercept = mean_erlang, linetype = 'dashed', color = 'red') + 
  scale_linetype_manual(values = 'dashed', name = "mean_erlang") 

```

(b) Assuming a $\textit{Gamma}$ prior such that the expected value is
    $\mu \ = \ 3$ with a standard deviation $\sigma \ = \ 1$,

-   Determine and draw the posterior distribution for $\mu$, given the
    data,
-   Evaluate mean, median and variance, both analytically and
    numerically in R.

```{r}
library(ggplot2)

mu_prior <- 3
sigma_prior <- 1

k <- (mu_prior / sigma_prior)^2 # evaluating considering a gamma prior
theta <- sigma_prior^2 / mu_prior

k_prime <- 26 # parametri della gamma posterior 
theta_prime <- 9 

x <- seq(0, 10, by = 0.01)  # Range of values for the Gamma distribution

prior_density <- function(x, shape, rate) {
  prior <- rgamma(x, shape = shape, rate = rate)
  return(prior)
}

posterior_density <- function(mu, data) { # the posterior is the product of a 
                                          # gamma dot a poisson likelihood 
  shape_prior <- k
  rate_prior <- theta
  prior <- dgamma(mu, shape = shape_prior, rate = rate_prior)
  likelihood <- prod(dpois(data, lambda = mu))
  posterior <- prior * likelihood
  return(posterior ) # Normalizing 
}


# --------------------- Plot the Gamma prior distribution -------------------------

prior_values <- dgamma(x, shape = k, rate = theta)


# -------------------------------------------------------

mu_values <- prior_density(x, k, theta) # extracting values from a gamma prior

hist(mu_values, breaks = 30, main = 'Probability Density Function for mu', xlab = expression(mu), ylab = 'Probability Density Function', col = 'red', freq = FALSE)
lines(x, mu_values, type = 'l', col = 'blue')
grid()

posterior_values <- sapply(x, posterior_density, data = N_par)
posterior_density_normalized <- posterior_values / sum(posterior_values)

# -------------------------------------------------------

N <- length(N_par)

## ----------------------- Calcolo analitico --------------------------------
## Media della posterior normalizzata 
## In questo caso i valori di mu seguono una distribuzione data dal prodotto 
## di una gamma prior e una poisson posterior 

mu_prime <- prior_density(x, k_prime, theta_prime) #posterior: is a gamma distribution with different parameters

#lambda <-  1 / theta_prime

#mean_analytical <- N * theta_prime
#median <- as.integer((1/theta_prime +1 ) / 2)
#median_value <- N_par[median]
#variance_analytical <- N/lambda^2

mean_analytical <- mean(mu_prime)
variance_analytical <- var(mu_prime)

## ------------------------  Calcolo numerico  ----------------------------------

# In questo caso i mu values dovrebbero essere generati dalla gamma prior 

mean_numerical <-  sum(mu_prime * posterior_values) / sum(posterior_values)

median_numerical <- quantile(mu_prime, probs = 0.5)[[1]]

variance_numerical <- sum((mu_prime^2 * posterior_values)) / sum(posterior_values) - mean_numerical^2


## ------------------------------------------------------------------------------

cat('Posterior Mean (Analytical):', mean_analytical, '\n')
#cat("Posterior Median (Analytical):", median_analytical, "\n")
cat('Posterior Variance (Analytical):', variance_analytical, '\n')

cat('----------------------------------------------------------------------- \n')

cat('Posterior Mean (Numerical):', mean_numerical, '\n')
cat('Posterior Median (Numerical):', median_numerical, '\n')
cat('Posterior Variance (Numerical):', variance_numerical, '\n')

## DOPO AVER AGGIUSTATO IL VALORE DELLA MEDIA AGGIUNGERLA SUL GRAFICO 

## ------------------Plot posterior distribution --------------------------

df3 <- data.frame(mu = x, posterior = posterior_density_normalized)

# Create the ggplot object
ggplot(df3, aes(x = mu, y = posterior)) +
  geom_point(color = 'firebrick', size = 0.7, alpha = 0.9, shape = 15) +
 geom_histogram(fill = "pink", stat = 'identity', alpha = 0.2) +  # Use geom_col for a histogram-like plot
  labs(x = expression(mu),
       y = 'Posterior Probability Density',
       title = paste('Posterior Distribution for', expression(mu) )) +
  xlim(0, 10) +
  ylim(0, max(posterior * 1.1)) + 
  theme_light() +  # Use a light theme with white background
  theme(panel.background = element_rect(fill = "white")) #+  
 # geom_vline(xintercept = mean_numerical, linetype = 'dashed', color = 'red') + 
 # scale_linetype_manual(values = 'dashed', name = "mean_numerical") 


```

(c) Evaluate a 95% credibility interval for the results obtained with
    different priors. Compare the result with that obtained using a
    normal approximation for the posterior distribution, with the same
    mean and standard deviation.

```{r}

# Function to calculate the cumulative posterior probability
cumulative_posterior <- cumsum(posterior_density_normalized)

# Find the lower and upper bounds of the 95% credibility interval
lower_bound <- min(mu_values[cumulative_posterior >= 0.025])
upper_bound <- max(mu_values[cumulative_posterior <= 0.975])

# Print the 95% credibility interval
cat(sprintf("95%% Credibility Interval (Gamma Prior) : [%.4f, %.4f]", lower_bound, upper_bound))

cat('\n')

# Calculate the mean and standard deviation of the posterior distribution
posterior_mean <- mean_analytical
posterior_sd <- sqrt(variance_analytical)

# Calculate the 95% credibility interval using normal approximation
normal_lower_bound <- qnorm(0.025, mean = posterior_mean, sd = posterior_sd)
normal_upper_bound <- qnorm(0.975, mean = posterior_mean, sd = posterior_sd)

# Print the 95% credibility interval using normal approximation
cat(sprintf("95%% Credibility Interval (Normal Approximation): [%.4f, %.4f]", normal_lower_bound, normal_upper_bound))



```

$\textbf{Exercise 2: Efficiency using Bayesian approach}$

A researcher A wants to evaluate the efficiency of detector 2 (Det2).
For this purpose, he sets up the apparatus shown in the figure 1, where
Det2 is sandwiched between Det1 and Det3. Let $\textbf{n}$ be the number
of signals recorded simultaneously by Det1 and Det3, and $\textbf{r}$ be
those also recorded by Det2, researcher $\textbf{A}$ obtains
$n \ = \ 500$ and $r \ = \ 312$.

![](images/Screenshot%202024-05-04%20220343.png)

Assuming a binomial model where $\textbf{n}$ is the number of trials and
$\textbf{r}$ is the number of successes out of $\textbf{n}$ trials:

(a) Evaluate the $\textit{mean}$ and the $\textit{variance}$ using a
    Bayesian approach under the hypothesis of:

-   uniform prior $\sim U(0,1)$;

-   Jeffrey's prior $\sim Beta(1/2, 1/2)$.

```{r}

## Considering first a uniform prior

n <- 500
r <- 312

alpha <- 1/2
beta <- 1/2

likelihood <- function(r, p, n) {
  dbinom(r, size = n, prob = p)
}

uniform_prior <- function(p) {
  dunif(p, min = 0, max = 1)
}

jeffreys_prior <- function(p) {
  dbeta(p, shape1 = 1/2, shape2 = 1/2)
}


posterior <- function(r, p, n, prior_function) {
       lklhd <- likelihood(r, p, n)   # binomial function
       prior <- prior_function(p)   # uniform or jeffreys
       post <- lklhd * prior
       return(post)
}

n_samples <- 201

p_values <- seq(0, 1, length.out = n_samples) # values used to generate the probability distribution 
delta <- 1 / n_samples


uniform_posterior_values <- posterior(r, p_values, n, uniform_prior) # generating posterior using a uniform prior
jeffreys_posterior_values <- posterior(r, p_values, n, jeffreys_prior) # generating posterior using a jeffreys prior 


## -------------------------- MEAN AND VARIANCE -----------------------------


calculate_posterior_mean_variance <- function(posterior_values, p) {
  
  posterior_values <- posterior_values / sum(posterior_values)
  
  posterior_mean <- sum(p * posterior_values)
  
  posterior_variance <- sum(p^2 * posterior_values) - (posterior_mean)^2 
  return(list(mean = posterior_mean, variance = posterior_variance))
}

alpha_uniform <- r + 1
beta_uniform <- n - r + 1
uniform_values_extracted <- rbeta(n_samples, alpha_uniform, beta_uniform)

uniform_mean_variance <- calculate_posterior_mean_variance(uniform_posterior_values, uniform_values_extracted)


alpha_jeffreys <- r + 1/2
beta_jeffreys <- n - r + 1/2 
jeffreys_values_extracted <- rbeta(n_samples, alpha_jeffreys, beta_jeffreys)

posterior_density <- density(jeffreys_values_extracted, from = 0, to = 1, n = n_samples)

# Normalize the density values to make them sum to 1
posterior_density$y <- posterior_density$y / sum(posterior_density$y)

# Compute the mean and variance using the posterior density
jeffreys_mean_variance <- calculate_posterior_mean_variance(posterior_density$y, posterior_density$x)

mean <- alpha /( alpha + beta) # Mean of the Jeffrey's prior distribution 
mode <- (alpha - 1)/( alpha + beta - 2)

cat("Uniform Prior (U(0, 1)):\n")
print(sprintf("Posterior Mean: %.4f", uniform_mean_variance$mean))
print(sprintf("Posterior Variance: %.4f", uniform_mean_variance$variance))

cat("Jeffrey's Prior (Beta(1/2, 1/2)):\n")
print(sprintf("Posterior Mean: %.4f", jeffreys_mean_variance$mean))
print(sprintf("Posterior Variance: %.4f", jeffreys_mean_variance$variance))


## -------------------------- PLOTTING ------------------------------

# Assuming p_values and uniform_posterior_values are your data vectors

# Plot the density line
plot(p_values, uniform_posterior_values, type = 'l', 
     main = 'Posterior Density Function (uniform prior)', 
     xlab = 'p', ylab = 'Posterior Density Function')

# Use polygon to fill the area under the curve
polygon(c(p_values, rev(p_values)), 
        c(uniform_posterior_values, rep(0, length(uniform_posterior_values))), 
        col=rgb(1, 0, 0, alpha=0.5), border="red")

# Add grid lines for better readability
grid()



plot(p_values, jeffreys_posterior_values, type = 'l', main = 'Posterior Density Function (Jeffrey\'s prior)', xlab = 'p', ylab = 'Posterior Density Function')
polygon(c(p_values, rev(p_values)), 
        c(jeffreys_posterior_values, rep(0, length(jeffreys_posterior_values))),
        col=rgb(0, 0, 1, alpha=0.5), border="blue")
grid()


p_prior_hist <- runif(201, 0, 1)
hist(p_prior_hist, freq = FALSE, col = 'lightpink', main = 'Uniform Prior Distribution', xlab = 'Value', ylab = 'Density Distribution Function')
curve(uniform_prior, add = TRUE, col = 'black', lwd = 2, n = 1000)


j_prior_hist <- rbeta(201, shape1 = alpha, shape2 = beta)
hist(j_prior_hist, freq = FALSE, col = "skyblue", main = "Jeffrey\'s Prior Distribution", xlab = "Value", ylab = "Density Distribution Function", xlim = c(0,1))
curve(jeffreys_prior, add = TRUE, col = "red", lwd = 2, n = 1000)
lines(c(mode , mode), c(0, 0.2), lty=2, lwd=2)
lines(c(mean , mean), c(0, 0.2), lty=2, lwd=2)

legend('topright', legend = expression(paste("Beta Prior Density (", alpha == 1/2, ", ", beta == 1/2, ")", sep = "")), col = "red", lwd = 2)

#print(paste('Mean =', mean, ' Mode = ', mode))

```

(b) Plot the posterior distributions for both cases.

Taking into account that the same detector has been studied by
researcher $\textbf{B}$, who has performed only $n \ = \ 10$
measurements and has obtained $r \ = \ 10$ signals:

(c) Evaluate the $\textit{mean}$, the $\textit{variance}$ and the
    $\textit{posterior distribution}$ using a uniform prior with the
    results of researcher $\textbf{B}$.

```{r}
nB <- 10 
rB <- 10 
    
uniform_posterior_valuesB <- posterior(rB, p_values, nB, uniform_prior)    
uniform_mean_varianceB <- calculate_posterior_mean_variance( uniform_posterior_valuesB, p_values)

plot(p_values, uniform_posterior_valuesB, type = 'l', main = 'Posterior Density Function (uniform prior)', xlab = 'p', ylab = 'Posterior Density Function')
polygon(c(p_values, rev(p_values)), 
        c(uniform_posterior_valuesB, rep(0, length(uniform_posterior_valuesB))),
        col=rgb(0, 0, 1, alpha=0.5), border="blue")
grid()

    
print(sprintf('Mean of the posterior distribution (researcher B): %.4f ', uniform_mean_varianceB$mean))
paste(sprintf('Variance of the posterior distribution (researcher B): %.4f', uniform_mean_varianceB$variance))
    
## CONTROLLARE I VALORI OTTENUTI 
    
```

(d) Repeat the computation of point (a) and (b) with the data of
    researcher $\textbf{A}$ using as a prior the posterior obtained from
    point (c).

```{r}
# Vedere se i valori sono corretti e rifare 
# Let's keep again the values from researcher A

n <- 500
r <- 312

new_prior <- uniform_posterior_valuesB # now the new prior is the posterior of the point above

new_post <- new_prior * likelihood(r, p_values, n)

new_post_mean <- calculate_posterior_mean_variance( new_post, p_values)

print(sprintf('Mean of the new posterior %.4f', new_post_mean$mean))
print(sprintf('Variance of the new posterior %.4f:', new_post_mean$variance))

plot(p_values, new_post, main = 'Posterior values', xlab = 'p', ylab = 'Probability Density Function', type = 'l')
polygon(c(p_values, rev(p_values)), 
        c(new_post, rep(0, length(new_post))),
        col=rgb(0, 1, 0, alpha=0.5), border="green")
grid()
```

(e) $\textbf{[Optional]}$ Compute $95%$ credible interval using the
    posterior of the previous point (d).

```{r}


# Function to calculate the cumulative posterior probability
#cumulative_posterior <- cumsum(new_post)

# Find the lower and upper bounds of the 95% credibility interval
#lower_bound <- min(p_values[cumulative_posterior >= 0.025])
#upper_bound <- max(p_values[cumulative_posterior <= 0.975])

# Print the 95% credibility interval
#cat("95% Credibility Interval (Gamma Prior): [", lower_bound, ", ", upper_bound, "]\n")





```

$\textbf{Exercise 3: Bayesian Inference for Binomial model}$

A coin is flipped $n \ = \ 30$ times with the following outcomes:
$\textit{T, T, T, T, T, H, T, T, H, H, T, T, H, H, H, T, H, T, H, T, H, H, T, H, T, H, T, H, H, H}$

(a) Assuming a flat prior, and a beta prior, plot the likelihood, prior
    and posterior distributions for the data set;

```{r}
library(ggplot2)

## Assuming first a flat prior 

n_tosses <- 30 
out <- c(TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE, TRUE, FALSE, FALSE, TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE)

tails <- sum(out == TRUE)
heads <- sum(out == FALSE)

## Defining a binomial likelihood distribution 

likelihood <- function(p) dbinom(heads, n, p)


## Defining a uniform prior 

flat_prior <- function(p) {dunif(p, 0,1)}

## Defining a beta prior 

beta_prior <- function(p) {dbeta(p, 5, 5)} # quali sono i valori di alpha e beta?

       
posterior <- function(p, likelihood, prior_func) {
  lklhd <- likelihood(p)
  prior <- prior_func(p)
  post <- lklhd * prior
  
  return(post)
}
  
p_values <- seq(0, 1, by = 0.001)

posterior_flat <- posterior(p_values, likelihood, flat_prior)
posterior_beta <- posterior(p_values, likelihood, beta_prior)

# Normalization constant
  
normalization_flat <- integrate(function(x) posterior(x, likelihood, flat_prior), 0, 1)$value
normalization_beta <- integrate(function(x) posterior(x, likelihood, beta_prior), 0, 1)$value

# Normalized posterior distributions
posterior_flat_normalized <- posterior_flat / normalization_flat
posterior_beta_normalized <- posterior_beta / normalization_beta


likelihood_values <- sapply(p_values, likelihood)
flat_prior_values <- sapply(p_values, flat_prior)
beta_prior_values <- sapply(p_values, beta_prior)
posterior_flat_values <- posterior_flat_normalized
posterior_beta_values <- posterior_beta_normalized

df_flat <- data.frame(mu = p_values, prior = flat_prior_values)
df_beta <- data.frame(mu = p_values, prior = beta_prior_values)
df_lklhd <- data.frame(mu = p_values, func = likelihood_values)
df_flat_post <- data.frame(mu = p_values, func = posterior_flat_values)
df_beta_post <- data.frame(mu = p_values, func = posterior_beta_values)
  
## ------------------------- Plotting ---------------------

plot_width <- 8  # Adjust the width as needed
plot_height <- 6  # Adjust the height as needed

likelihood_plot <- ggplot(df_lklhd, aes(x = mu, y = func)) +
  geom_line(aes(x = p_values, y = likelihood_values), color = "blue", size = 1) +
  labs(title = "Likelihood Distribution", x = "Probability of Heads (p)", y = "Density") +
   theme_minimal() 

flat_prior_plot <- ggplot(df_flat, aes(x = mu, y = prior)) +
  geom_line(aes(x = p_values, y = flat_prior_values), color = "green", size = 1) +
  labs(title = "Flat Prior Distribution", x = "Probability of Heads (p)", y = "Density") +
  theme_minimal()

beta_prior_plot <- ggplot(df_beta, aes(x = mu, y = prior)) +
  geom_line(aes(x = p_values, y = beta_prior_values), color = "orange", size = 1) +
  labs(title = "Beta Prior Distribution", x = "Probability of Heads (p)", y = "Density") +
  theme_minimal()

posterior_flat_plot <- ggplot(df_flat_post, aes(x = mu, y = func)) +
  geom_line(aes(x = p_values, y = posterior_flat_values), color = "red", size = 1) +
  labs(title = "Posterior Distribution (Flat Prior)", x = "Probability of Heads (p)", y = "Density") +
  theme_minimal()

posterior_beta_plot <- ggplot(df_beta_post, aes(x = mu, y = func)) +
  geom_line(aes(x = p_values, y = posterior_beta_values), color = "purple", size = 1) +
  labs(title = "Posterior Distribution (Beta Prior)", x = "Probability of Heads (p)", y = "Density") +
  theme_minimal()


#Arrange plots in a grid
#grid.arrange(likelihood_plot, flat_prior_plot, beta_prior_plot,
#            posterior_flat_plot, posterior_beta_plot,
 #          ncol = 2, nrow = 3)

likelihood_plot
flat_prior_plot
beta_prior_plot
posterior_flat_plot
posterior_beta_plot


```

As we can observe from the plot, the two posterior functions don't
differ so much. That can be due to the fact that the beta and uniform
prior distributions are similar.

(b) Evaluate the most probable value for the coin probability $p$ and,
    integrating the posterior probability distribution, give an estimate
    for a $95 \%$ credibility interval;

```{r}

## Let's first find the mode of the posterior probability density 
mode_index <- which.max(posterior_flat_values) #valore maggiore della posterior
mode_p <- p_values[mode_index]

cat('Most probable value for p:', mode_p, '\n')

cumulative_posterior <- cumsum(posterior_flat_values) # estimating the cumulative function 

lower_bound <- min(p_values[cumulative_posterior >= 0.025])
upper_bound <- max(p_values[cumulative_posterior <= 0.975])

cat(sprintf('95%% credibility interval: [%.3f, %.3f]', lower_bound,  upper_bound))

```

(c) Repeat the same analysis assuming a sequential analysis of the data.
    Show how the most probable value and the credibility interval change
    as a function of the number of coin tosses (i.e. from 1 to 30);

```{r}

## Let's now perform a sequential analysis of the data 
modes <- numeric(n_tosses)
lower_bounds <- numeric(n_tosses)
upper_bounds <- numeric(n_tosses)

for (i in 1:n_tosses) {
  data <- out[1:i]
  

  likelihood <- function(p) dbinom(sum(data == FALSE), length(data), p)
  
  posterior <- function(p) likelihood(p) * dbeta(p, 1, 1) #assuming a flat prior 
  
  p_values <- seq(0, 1, by = 0.001)
  posterior_values <- posterior(p_values)
  mode_index <- which.max(posterior_values)
  mode_p <- p_values[mode_index]
  modes[i] <- mode_p
  
  minimum_integral <- 1
  for (p in modes){
         integral <- abs(integrate(posterior, lower = 0, upper = p)$value - 0.025)
  
  if (integral < minimum_integral) {
    minimum_integral <- integral
    lower_bound <- p
  }
  lower_bounds[i] <- lower_bound
  }
  
  minimum_integral <- 1
  for (p in modes) {
    integral <- abs(integrate(posterior, lower = 0, upper = p)$value - 0.975)
    
  if (integral < minimum_integral) {
    minimum_integral <- integral
    upper_bound <- p
  }
    
    
  upper_bounds[i] <- upper_bound
  }
  
}

# Plot results
plot(1:n_tosses, modes, type = "l", ylim = c(0, 1), xlab = "Number of Tosses", ylab = "Probability of Heads", main = "Sequential Analysis")
lines(1:n_tosses, lower_bounds, col = "blue", lty = 2)
lines(1:n_tosses, upper_bounds, col = "blue", lty = 2)
grid()
legend("topright", legend = c("Mode", "95% Credibility Interval"), col = c("black", "blue"), lty = c(1, 2))

```

(d) Do you get a different result, by analyzing the data sequentially
    with respect to a one-step analysis (i.e. considering all the data
    as a whole)?

```{r}


cat(sprintf('The most probable value for p in this case is %.2f',  modes[length(modes)]))




```

As a consequence, we can conclude that we obtain a different result
analyzing the data sequentially or in a one-step analysis.

$\textbf{Exercise 4: Poll}$

A couple of days before an election in which four parties (A, B, C, D)
compete, a poll is taken using a sample of 200 voters who express the
following preferences: 57, 31, 45 and 67 for, respectively, parties A,
B, C and D.

Using a Bayesian approach, for all parties:

(a) Calculate the expected percentage of votes and a $68 \%$ credibility
    interval by assuming as prior a:

-   uniform prior;
-   a prior constructed from the results obtained from another poll
    conducted the previous week on a sample of 100 voters who expressed
    the following preferences 32, 14, 26 and 28 for, respectively,
    parties A, B, C and D.

(b) Sample size to obtain a margin of error less or equal than
    $\pm \ 3 \%$ for each party.

```{r}

N_voters <- 200
preferences <- c(51, 31, 45, 67)

N2_voters <- 100
preferences2 <- c(32, 14, 26, 28)

## The prior is related to the probability of having a given vote to a party, 
## using a uniform prior we mean that the probability of a vote to a section is 
## uniform, there's no preferred party (so it's related to the preferences)

alpha_uniform <- 1 + preferences
beta_uniform <- 1 + N_voters - preferences

# Calculate expected percentage of votes
expected_percentage_uniform <- (alpha_uniform / (alpha_uniform + beta_uniform)) * 100


lower_ci_uniform <- qbeta(0.16, alpha_uniform, beta_uniform) * 100
upper_ci_uniform <- qbeta(0.84, alpha_uniform, beta_uniform) * 100

# Print results
cat("Uniform Prior:\n")
formatted_string <- sprintf("Expected Percentage of Votes for each party: %.2f%%, %.2f%%, %.2f%%, %.2f%%", 
                            expected_percentage_uniform[1], 
                            expected_percentage_uniform[2], 
                            expected_percentage_uniform[3], 
                            expected_percentage_uniform[4])

# Use paste if you need to concatenate with other strings
result <- paste(formatted_string)

# Print the result
cat(result)

cat('\n')
cat("68% Credibility Interval:\n")
cat(paste(sprintf("Party A: [%.2f%% - %.2f%%]", lower_ci_uniform[1], upper_ci_uniform[1])), '\n')
cat(paste(sprintf("Party B: [%.2f%% - %.2f%%]", lower_ci_uniform[2], upper_ci_uniform[2])), "\n")
cat(paste(sprintf("Party C: [%.2f%% - %.2f%%]", lower_ci_uniform[3],  upper_ci_uniform[3])), "\n")
cat(paste(sprintf("Party D [%.2f%% - %.2f%%]:", lower_ci_uniform[4], upper_ci_uniform[4])),  "\n")

alpha_uniform2 <- 1 + preferences2
beta_uniform2 <- 1 + N2_voters - preferences2

# Calculate expected percentage of votes
expected_percentage_uniform2 <- (alpha_uniform2 / (alpha_uniform2 + beta_uniform2)) * 100


lower_ci_uniform2 <- qbeta(0.16, alpha_uniform2, beta_uniform2) * 100
upper_ci_uniform2 <- qbeta(0.84, alpha_uniform2, beta_uniform2) * 100

# Print results
cat("Next poll:\n")
formatted_string <- sprintf("Expected Percentage of Votes for each party: %.2f%%, %.2f%%, %.2f%%, %.2f%%", 
                            expected_percentage_uniform2[1], 
                            expected_percentage_uniform2[2], 
                            expected_percentage_uniform2[3], 
                            expected_percentage_uniform2[4])

# Use paste if you need to concatenate with other strings
result <- paste(formatted_string)

# Print the result
cat(result)
cat('\n')
cat("68% Credibility Interval:\n")
cat(paste(sprintf("Party A: [%.2f%% - %.2f%%]", lower_ci_uniform2[1], upper_ci_uniform2[1])), '\n')
cat(paste(sprintf("Party B: [%.2f%% - %.2f%%]", lower_ci_uniform2[2], upper_ci_uniform2[2])), "\n")
cat(paste(sprintf("Party C: [%.2f%% - %.2f%%]", lower_ci_uniform2[3],  upper_ci_uniform2[3])), "\n")
cat(paste(sprintf("Party D [%.2f%% - %.2f%%]:", lower_ci_uniform2[4], upper_ci_uniform2[4])),  "\n")


```

To calculate the sample size required to achieve a margin of error of
$\pm \ 3\%$ for each party, we need to use the formula for the sample
size in a Bayesian context. In Bayesian statistics, sample size
determination involves specifying a prior distribution for the parameter
of interest, updating this prior with observed data to obtain a
posterior distribution. We assume a uniform prior distribution for each
party's proportion of votes. The formula to calculate the sample size
required is:

$$
n_i = \frac{(\Phi^{-1}(1 - \alpha/2))^2 \cdot \hat{p_i} \cdot (1 - \hat{p_i})}{(\text{Margin of Error} \cdot \hat{p_i})^2}
$$

```{r}
# Define the observed proportions for each party
propr_A <- 57/200 + 32/100
propr_B <- 31/200 + 14/100
propr_C <- 45/200 + 26/100
propr_D <- 67/200 + 28/100

observed_proportions <- c(propr_A, propr_B, propr_C, propr_D)
#observed_proportions <- c(57/200, 31/200, 45/200, 67/200)

# Define the desired margin of error
margin_of_error <- 0.03

# Define the significance level
alpha <- 0.05 # probabilità massima che un test statistica commetta un errore
              # di default posto al 5%

# Define the quantile function for the standard normal distribution
quantile_normal <- qnorm(1 - alpha/2)

# Function to calculate sample size for each party
calculate_sample_size <- function(observed_proportion) {
  numerator <- (quantile_normal^2) * observed_proportion * (1 - observed_proportion)
  denominator <- (margin_of_error * observed_proportion)^2
  sample_size <- numerator / denominator
  return(ceiling(sample_size))
}

# Calculate sample sizes for each party
sample_sizes <- sapply(observed_proportions, calculate_sample_size)

# Print the results
party_names <- c("A", "B", "C", "D")
for (i in 1:length(sample_sizes)) {
  cat("Sample size for Party", party_names[i], ":", sample_sizes[i], "\n")
}

```
