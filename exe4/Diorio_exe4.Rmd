---
title: "Exercise_4_Adv_statistics"
author: "Ada D'Iorio"
output: 
html_document: 
number_sections: true 
theme: spacelab 
pdf_document:
    latex_engine: xelatex
date: "2024-05-04"
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(scales)
```

**Exercise 1**

-   A well established and diffused method for detecting a disease in
    blood fails to detect the presence of disease in 15% of the patients
    that actually have the disease.

-   A young UniPD startUp has developed an innovative method of
    screening. During the qualification phase, a random sample of n = 75
    patients known to have the disease is screened using the new method.

(a) What is the probability distribution of y, the number of times the
    new method fails to detect the disease?

```{r}

prob_failure <- 0.15 # probability of failure is of 15%
prob_suc <- 1 - prob_failure

n <- 75 # patients with the disease screened 


freq_prob <- 6 / 75 # percentage of failures 
                    # binomial distribution -> yes because we have true or false 

x <- 0:100 

y <- rbinom(x, n, prob_failure) # values extracted from a binomial distribution
pdf <- dbinom(x, n, prob_failure)
#pdf <- pdf * length(y) / sum(pdf)

hist(y, breaks = 20, col = 'lightcoral', xlab = 'Values', main = 'Probability distribution for y', freq = FALSE)
lines(x, pdf, type = 'l', lwd = 2)
grid()
```

The probability distribution for y is a binomial function with a number
of successes $n \ = \ 75$ and a probability of success $p \ = \ 0.15$.

(b) On the $n =75$ patients sample, the new method fails to detect the
    disease in y = 6 cases. What is the frequentist estimator of the
    failure probability of the new method?

```{r}

cat(sprintf('The failure probability in the frequentist approach is of the %.1f%%', freq_prob * 100))
```

(d) Perform a test of hypothesis assuming that if the probability of
    failing to the detect the disease in ill patients is greater or
    equal than 15%, the new test is no better that the traditional
    method. Test the sample at a 5% level of significance in the
    Bayesian way;

```{r}

## ---- Bayesian approach ------

estBetaParams <- function(mu, var) {
  alpha <- ((1 - mu) / var - 1 / mu) * mu ^ 2
  beta <- alpha * (1 / mu - 1)
  return(params = list(alpha = alpha, beta = beta))
}

n <- 75 
x <- 6 # numero di successi 
  
mean_beta <- 0.15 # beta prior for the successes
sd_beta <- 0.14
alfa_prior <- estBetaParams(mean_beta, sd_beta^2)$alpha
beta_prior <- estBetaParams(mean_beta, sd_beta^2)$beta

# Binomial likelihood
# New beta posterior 
alfa_posterior <- alfa_prior + x # x number of successes
beta_posterior <- beta_prior + n - x
p_values <- seq(0, 1, 0.001)

formatted_alfa_posterior <- sprintf('%.2f', alfa_posterior)
formatted_beta_posterior <- sprintf('%.2f', beta_posterior)

mean_posterior <- alfa_posterior / (alfa_posterior + beta_posterior)
var_posterior <- (alfa_posterior * beta_posterior) / ((alfa_posterior + beta_posterior)^2 * (alfa_posterior + beta_posterior + 1))
sd_posterior <- sqrt(var_posterior)

posterior_values <- rbeta(p_values, alfa_posterior, beta_posterior)
plot_post <- dbeta(p_values, alfa_posterior, beta_posterior)



plot(p_values, plot_post, type = 'l', lwd = 2, col  ='blue', xlim = c(0, 0.25), main = 'Posterior probability distribution', xlab = 'p', ylab = 'Probability density')
abline(v = mean_posterior, col = 'red', lwd = 2, lty = 2)
#text(plot_post[length(plot_post)-100], max(plot_post), labels = sprintf('Mean = %.2f', mean_posterior), pos = 4, col = 'black')
arrows(mean_posterior, 10, mean_posterior + sd_posterior / 2, 10, length = 0.1, angle = 20, col = 'red')
arrows(mean_posterior + sd_posterior / 2, 10, mean_posterior, 10, length = 0.1, angle = 20, col = 'red')
text(0.15, 5, labels = bquote(sigma == .(round(sd_posterior, 2))), col = 'red', pos = 3)



text(0.15, 7, labels = c(bquote(alpha == .(formatted_alfa_posterior))))
text(0.15, 9, labels = c(bquote(beta == .(formatted_beta_posterior))))

grid()


legend('topright', 
       legend = c('Mean', 'Variance'), col = c('black', 'red'), lwd = 2, lty = c(2,2))

cat(sprintf('Parameters for the beta prior [alpha, beta]: [%.2f, %.2f]', alfa_prior, beta_prior))
cat('\n')
cat(sprintf('Parameters for the beta posterior [alpha, beta]: [%.2f, %.2f]', alfa_posterior, beta_posterior))


```

(d) Perform a test of hypothesis assuming that if the probability of
    failing to the detect the disease in ill patients is greater or
    equal than 15%, the new test is not better that the traditional
    method. Test the sample at a 5% level of significance in the
    Bayesian way;

```{r}

## Setup a null hypothesis: H_0: p >= 0.15, the new test is better than 
## the previous one 
## Alternative hypothesis: the new test is not better than the 
## previous one: H_1: p < 0.15 
## We want to prove that the NULL hypothesis is indeed FALSE

## The null distribution of the test statistic is the sampling distribution
## of the test statistics, given that the null hypothesis is true

## In the bayesian approach we evaluate the posterior probability of the null 
## hypothesis, and integrate over the required region


alpha <- 0.5

# Choosing a level of significance of 5%
plot(p_values, plot_post, type = 'l', lwd = 2, main = 'Posterior probability distribution', ylab = 'Probability', xlab = 'p', xlim = c(0, 0.25))

grid()
polygon(c(p_values[p_values >= 0.15], 0.15),
        c(dbeta(p_values, alfa_posterior, beta_posterior)[p_values >= 0.15], 0),
        col = alpha('blue', 0.25),
        border = 1)


result <- integrate(function(p) { dbeta(p, alfa_posterior, beta_posterior) }, 0.15, 1)$value

paste(sprintf('The integrated area is equal to %.2f', result))

# We can conclude that the new method is better because we can reject the null 
# hypothesis with a 5% level of significance

```

Given the calculation, we can observe that the value evaluated for the
area is of $0.03 \ < 0.05$. As a consequence, in the Bayesian approach
we can neglect the null hypothesis.

(e) Perform the same hypothesis test in the classical frequentist way.

```{r}

## FREQUENTIST APPROACH 
# Computing the cumulative function 

p_value <- pbinom(6, n, 0.15)

print(sprintf('The p-value is equal to %.3f ', p_value))


```

In this case, we obtain $p-value \ = \ 0.054 > \ 0.05$, so the result
lies in the acceptance region. In the frequentist approach, we cannot
neglect the null hypothesis.

**Exercise 2**

-   A researcher has collected \$ n   =  16 \$ observations that are
    supposed to come from a Normal distribution with known variance
    $\sigma^2 \ = \ 4$:

\$  4.09  4.68  1.87  2.62  5.58  8.68  4.07  4.78 \\ 4.79  4.49  5.85 
5.09  2.40  6.27 6.30  4.47 \$

-   Assuming the prior is a step function:

$$
g(\mu) = \begin{cases}
\mu & \text{for } 0 < \mu \le 3, \\
3 & \text{for } 3 < \mu \le 5, \\
8 - \mu & \text{for } 5 < \mu \le 8, \\
0 & \text{for } \mu > 8
\end{cases}
$$

(a) Find the posterior distribution, the posterior mean and standard
    deviation;

    ```{r}

     
      step_function <- function(mus) {
       out <- vector('numeric', length = length(mus))
       
       for (i in 1:length(mus)) {
         mu <- mus[[i]]  
         if (mu > 0 & mu <= 3) {
            out[[i]] <- mu
      }
      if (mu > 3 & mu <= 5) {
        out[[i]] <- 3
      }
      if (mu > 5 & mu <= 8) {
         out[[i]] <- 8 - mu
      }
      if (mu > 8) {
        out[[i]] <- 0
      }
       }
       return(out)
      }


      sigma_norm <- 2
      n <- 16 # number of observations 
      values <- c(4.09, 4.68, 1.87, 2.62, 5.58, 8.68, 4.07, 4.78, 4.79, 4.49, 5.85, 5.09, 2.40, 6.27, 6.30, 4.47)

      mu_values <- seq(0,10,by=0.01) # generating linear values in 0:10
      x <- 0:10 
      
      posterior_density <- function(mus) {
      post <- double(length= length(mus))
      priors <- double(length = length(mus))
      lklhd <- double(length = length(mus))
      
        for (i in 1:length(mus)) {
          mu <- mus[i]
          
       prior <- step_function(mu)
       likelihood <- prod(dnorm(values, mean = mu, sd= sigma_norm))
       
       
       priors[i] <- prior
       lklhd[i] <- likelihood
       post[i] <- prior * likelihood 
       
       
       
        }
      post <- post / sum(post)
      lklhd <- lklhd / sum(lklhd)
      priors <- priors / sum(priors)
      
        return(list(posterior = post, prior = priors, likelihood = lklhd))
      }
      

     
     posterior <- posterior_density(mu_values)$posterior
     
     # Estimating the mean of the posterior distribution 
     
     mean <- sum(mu_values * posterior)
     variance <- sum((mu_values - mean)^2 * posterior) 
     sd <- sqrt(variance)
     
     cat(sprintf('The mean for the posterior distribution is: %.2f', mean), '\n')
     cat(sprintf('The variance for the posterior distribution is: %.2f', variance))
     
     plot(mu_values, posterior, type = 'l', col = 'blue', lwd = 2, main = 'Posterior probability function', ylab = 'Probability', xlab = expression(mu))
     
     
     grid()


    ```

(b) Find the 95% credibility interval for $\mu$;

```{r}

cumulative_posterior <- cumsum(posterior)   
  
lower_bound <- min(mu_values[cumulative_posterior >= 0.025])
upper_bound <- max(mu_values[cumulative_posterior <= 0.975])

# Print the 95% credibility interval
cat(sprintf("95%% Credibility Interval: [%.2f, %.2f]", lower_bound, upper_bound))
   
   
```

(c) Plot the posterior distribution, indicating on the same plot: the
    mean value, the standard deviation, and the $95 \%$ credibility
    interval;

```{r}

plot(mu_values, posterior, type = 'l', col = 'blue', lwd = 2, main = 'Posterior probability function', ylab = 'Probability', xlab = 'p')
abline(v = mean, col = 'red', lwd = 2, lty = 2)
#abline(v = c(lower_bound, upper_bound), col = "lightblue", lwd = 2, lty = 2)
legend("topright", legend = c("Mean", "95% Credibility Interval"),
       col = c("red", alpha('blue', 0.25)), lty = c(2,1), lwd = 2)
     
grid()

area_x <- c(lower_bound, mu_values[mu_values >= lower_bound & mu_values <= upper_bound], upper_bound)
area_y <- c(0, posterior[mu_values >= lower_bound & mu_values <= upper_bound], 0)

arrows(mean + sd / 2, 0.004, mean + sd, 0.004, length = 0.1, angle = 20, col = 'red')
arrows(mean + sd/ 2, 0.004, mean , 0.004, length = 0.1, angle = 20, col = 'red')
text(mean_posterior + 2.5, 0.004, labels = bquote(sigma == .(round(sd_posterior, 2))), col = 'red', pos = 3)

polygon(area_x, area_y, col = alpha('blue', 0.25))


```

(d) Plot, on the same graph, the prior, the likelihood and the posterior
    distribution.

```{r}
prior <- posterior_density(mu_values)$prior
likelihood <- posterior_density(mu_values)$likelihood

plot(mu_values, posterior, type = 'l', col = "dodgerblue"
, lwd = 2, main = 'Probability distribution functions', ylab = 'Probability', xlab = expression(mu))
lines(mu_values, prior, col = "orchid", lwd = 2)
lines(mu_values, likelihood, col = "forestgreen", lwd = 2)
legend('topright', legend = c('Prior', 'Likelihood', 'Posterior'), col = c("orchid", "forestgreen", "dodgerblue"
), lty = 1, lwd = 2)
grid() 



```

**Exercise 3**

The six boxes toy model is described in reference \textbf{[1]}.

-   Labeling the boxes as follows:

    ![](images/Screenshot%202024-05-13%20195727.png)

-   Write a program in R that:

1)  Selects a random box;
2)  Makes a random sampling from the box;
3)  Prints on the standard output the probability of selecting each box;
4)  Plots the probability for each box as a function of the number of
    trials.

```{r}

  
  
sample_from_box <- function(box) {
  prob_white <- c(1/6, 2/6, 3/6, 4/6, 5/6, 6/6)
  rbinom(1, 1, prob_white[box + 1])
}
  



update_probabilities <- function(prior_probabilities, result) {
  prob_white <- c(1/6, 2/6, 3/6, 4/6, 5/6, 6/6)
  
  likelihood <- numeric(6)
  
  for (i in 1:length(prob_white)) {
    likelihood[i] <- ifelse(result == 1, prob_white[i], 1 - prob_white[i])
  }

  #likelihood <- ifelse(result == 1, prob_white, 1 - prob_white)
  
  # restituisce prob_white se il risultato è 1, 1 - prob_white se il risultato è 0
 
  posterior_probabilities <- prior_probabilities * likelihood
  # prodotto di un vettore iniziale (prior_probabilities) per la likelihood che è anche essa un vettore 
  
  posterior_probabilities <- posterior_probabilities / sum(posterior_probabilities)
  
  
  return(posterior_probabilities)
}



random_sampling <- function(num_trials, boxes) {
  probabilities <- rep(1/6, 6)
  plot_data <- matrix(nrow = num_trials+1, ncol = 6)
  
  
  plot_data[1,] <- probabilities
  
  set.seed(123)
  
  for (i in 1:num_trials) {
    
    box_selected <- sample(0:5, 1)
    item <- sample_from_box(box_selected)
    probabilities <- update_probabilities(plot_data[i,], item)
    
    plot_data[i+1, ] <- probabilities
    
    
  }
  
  par(mfrow = c(3, 2), mar = c(3, 3, 2, 1))
  
  for (i in 1:6) {
  plot(1:(num_trials+1), plot_data[,i], lty = 1, col = i,
          xlab = 'Numero di prove', ylab = 'Probabilità',
          main = paste('Probability for H', i), ylim = c(0,1))
  grid()
  }
  par(mfrow = c(1, 1))
  
  paste('--------------------------------------------------')
  paste('Probabilities for:')
  cat(paste(sprintf('H_0 \ H_1 \ H_2 \ H_3 \ H_4 \ H_5 \ H_6 \n')))
  print(plot_data)
}



# Numero di prove
num_trials <- 80

# Esecuzione del campionamento casuale e plottaggio delle probabilità
random_sampling(num_trials, boxes)



```

Let's now explain what is happening in the simulation. At the first
stage all the boxes are considered equally likely, assigning $1/6$
probability to each of them. Also the black and white balls are
considered with probabilities of $1/2$. Going on with the experiment we
discover that the probability of picking a white ball from a box has to
be proportional to the number of white balls of each hypothetical
composition. The updating rule can be defined as:

$$ P(H_? = H_i | E^i, I) \propto \pi_i $$ where $\pi_i = i/N$, with $N$
the total number of balls in box $i$ and $I$ stands for all the
background information available for the experiment. In general the
updating rule is given by Bayes' theorem:
$$ P(E^i | I) \propto \sum_i P(E^i | H_i, I) P(H_i | I) $$
